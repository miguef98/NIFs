{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si $f_\\theta : \\mathbb{R}_{\\geq 0} \\times \\Omega \\rightarrow \\mathbb{R}$ es la red neuronal: $f_\\theta( t, \\vec{x})$\n",
    "\n",
    "Una opcion es minimizar el BE? Pero aca esta fijo el $t$\n",
    "\n",
    "$$ \\underset{u: \\Omega \\rightarrow \\mathbb{R}}{\\text{min}} \\quad \\quad \\int_\\Omega \\left | t(id - \\nabla \\cdot \\circ \\, P_{\\nabla \\psi} \\,\\nabla) f_\\theta - u_0 \\right | \\quad d\\vec{x}.$$\n",
    "\n",
    "La otra es la mejor... que es minimizar la ecuacion del calor de verdad, pero intrinseco a $\\partial \\Omega$:\n",
    "\n",
    "$$\\begin{matrix} & \\frac{\\partial u}{\\partial t} &= \\Delta_{\\partial \\Omega} u \\\\ \\text{sujeto a:} & u(\\gamma) &= \\delta(\\gamma) \\end{matrix}$$\n",
    "\n",
    "Con $\\delta(\\cdot)$ la funcion delta de Dirac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from src.model import SIREN\n",
    "import random\n",
    "import src.diff_operators as dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDF_model = SIREN(\n",
    "        n_in_features= 3,\n",
    "        n_out_features=1,\n",
    "        hidden_layer_config=[512]*4,\n",
    "        w0=30,\n",
    "        ww=None,\n",
    "        activation= 'sine'\n",
    ")\n",
    "SDF_model.load_state_dict( torch.load('results/torus/experiment/models/model_best.pth', weights_only=True))\n",
    "\n",
    "device = torch.device(0)\n",
    "SDF_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123 \n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "heat_kernel = SIREN(\n",
    "    n_in_features= 4, # tenemos 4 porque depende del tiempo tambien\n",
    "    n_out_features=1,\n",
    "    hidden_layer_config=[16] * 4,\n",
    "    w0=30,\n",
    "    ww=None,\n",
    "    activation='sine'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_kernel.to(device)\n",
    "\n",
    "losses = dict()\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "current_lr = 1e-4\n",
    "epochs = 1000\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "            lr=current_lr,\n",
    "            params=heat_kernel.parameters()\n",
    "        )\n",
    "\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = current_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data():\n",
    "    boundary_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # zero the parameter gradients\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    boundary_data, domain_data = \n",
    "    # forward + backward + optimize\n",
    "    input_data = input_data.to( device )\n",
    "    normals = normals.to(device)\n",
    "    sdfs = sdfs.to(device)\n",
    "    \n",
    "    loss = loss_fn( \n",
    "        model, \n",
    "        input_data, \n",
    "        {'normals': normals, 'sdfs': sdfs}, \n",
    "        loss_weights\n",
    "    )\n",
    "\n",
    "    wandb.log( loss )\n",
    "\n",
    "    train_loss = torch.zeros((1, 1), device=device)\n",
    "    for it, l in loss.items():\n",
    "        train_loss += l\n",
    "        # accumulating statistics per loss term\n",
    "        if it not in running_loss:\n",
    "            running_loss[it] = l.item()\n",
    "        else:\n",
    "            running_loss[it] += l.item()\n",
    "\n",
    "    train_loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # accumulate statistics\n",
    "    for it, l in running_loss.items():\n",
    "        if it in losses:\n",
    "            losses[it][epoch] = l\n",
    "        else:\n",
    "            losses[it] = [0.] * epochs\n",
    "            losses[it][epoch] = l\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for k, v in running_loss.items():\n",
    "        epoch_loss += v\n",
    "    epoch_loss /=+ dataset.batchesPerEpoch\n",
    "    print(f\"Epoch: {epoch} - Loss: {epoch_loss} - Learning Rate: {current_lr:.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from src.model import SIREN\n",
    "from src.obj import load\n",
    "import meshplot as mp\n",
    "import src.diff_operators as dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SIREN(\n",
    "        n_in_features= 3,\n",
    "        n_out_features=1,\n",
    "        hidden_layer_config=[512]*4,\n",
    "        w0=30,\n",
    "        ww=None,\n",
    "        activation= 'sine'\n",
    ")\n",
    "model.load_state_dict( torch.load('results/bunny/experiment/models/model_best.pth', weights_only=True))\n",
    "\n",
    "device_torch = torch.device(0)\n",
    "model.to(device_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc( model, samples, max_batch=64**2, device=torch.device(0), iterations = 3 ):\n",
    "    # samples = ( amount_samples, 3 )\n",
    "    amount_samples = samples.shape[0]\n",
    "\n",
    "    head = 0\n",
    "    X = samples.copy()\n",
    "\n",
    "    for i in range( iterations):\n",
    "        print(f'Iteration: {i}')\n",
    "        mean_distance = 0\n",
    "        while head < amount_samples:\n",
    "            \n",
    "            inputs_subset = torch.from_numpy( X[head:min(head + max_batch, amount_samples), :] ).to(device).unsqueeze(0).float()\n",
    "\n",
    "            x, y = model(inputs_subset).values()\n",
    "\n",
    "            mean_distance += torch.sum(y).detach().cpu().numpy()\n",
    "\n",
    "            grad_psi = torch.nn.functional.normalize( dif.gradient(y,x), dim=-1 )\n",
    "\n",
    "            X[head:min(head + max_batch, amount_samples)] -= (y *  grad_psi).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "            head += max_batch\n",
    "\n",
    "        print(f'Mean distance: { mean_distance / amount_samples }')\n",
    "\n",
    "    return X\n",
    "\n",
    "pc, _ = load( 'data/bunny/bunny_pc.obj' )\n",
    "#pc = pc[np.random.choice( np.arange(100000), 10000),: ]\n",
    "pc = grad_desc( model, pc, iterations=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "EPSILON = 0.001\n",
    "\n",
    "u0 = lambda x, p: torch.exp( - torch.sum( (x - p)**2, dim=-1) / (EPSILON ** 2) )[...,None] / ( EPSILON )\n",
    "grad_u0 = lambda x, p: (-2 / (EPSILON ** 2)) * u0(x, p) * (x - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project( projectees, projectors ):\n",
    "    # asumiendo ||projectors|| = 1\n",
    "    return projectees -  torch.sum(projectors * projectees, dim=-1)[...,None] * projectors\n",
    "\n",
    "def computeX( model, samples, p, u0, grad_u0, max_batch=64**2, device=torch.device(0) ):\n",
    "    # samples = ( amount_samples, 3 )    \n",
    "    head = 0\n",
    "    amount_samples = samples.shape[0]\n",
    "\n",
    "    uts = np.zeros( (amount_samples, 1))\n",
    "    grads_psi = np.zeros( (amount_samples, 3))\n",
    "    laplacians_psi = np.zeros( (amount_samples, 1))\n",
    "    X = np.zeros( (amount_samples, 3))\n",
    "    divX = np.zeros( (amount_samples, 1))\n",
    "\n",
    "    ps = torch.from_numpy( np.tile(p, (max_batch, 1)) ).to(device_torch)\n",
    "    i = 0\n",
    "    while head < amount_samples:\n",
    "        print(f'Iteration: {i}')\n",
    "        \n",
    "        inputs_subset = torch.from_numpy( samples[head:min(head + max_batch, amount_samples), :] ).to(device).unsqueeze(0).float()\n",
    "\n",
    "        x, y =  model(inputs_subset).values()\n",
    "\n",
    "        grad_psi = dif.gradient(y,x)\n",
    "\n",
    "        ps_ss = ps[:inputs_subset.shape[1],:].unsqueeze(0)\n",
    "\n",
    "        # Forward Euler\n",
    "        proj_grad_u0 = project( grad_u0( inputs_subset, ps_ss ), grad_psi )\n",
    "        F_u0 = dif.divergence( proj_grad_u0 , x )\n",
    "        u0s = u0( inputs_subset, ps_ss )\n",
    "        ut = u0s + t * F_u0\n",
    "\n",
    "        # RK4: ( out of memory... lo mata hacer el gradiente de u para calcular X.. es un monton)\n",
    "        #k1 = dif.divergence( project( grad_u0( inputs_subset, ps_ss ), grad_psi ) , x )\n",
    "        #k2 = dif.divergence( project( dif.gradient(u0s + 0.5 * t * k1 , x), grad_psi ), x )\n",
    "        #k3 = dif.divergence( project( dif.gradient(u0s + 0.5 * t * k2 , x), grad_psi ), x )\n",
    "        #k4 = dif.divergence( project( dif.gradient(u0s + t * k3 , x), grad_psi ), x )\n",
    "        #ut = u0s + (t/6) * (k1 + 2*k2 + 2*k3 + k4)\n",
    "\n",
    "        # Heuns:\n",
    "        #k1 = dif.divergence( proj_grad_u0 , x )\n",
    "        #k2 = dif.divergence( project( dif.gradient(u0s + t * k1 , x), grad_psi ), x )\n",
    "        #ut = u0s + (t/2) * (k1 + k2)\n",
    "        \n",
    "        uts[head:min(head + max_batch, amount_samples)] = ut.detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "        #X_ss = -1 * torch.nn.functional.normalize( proj_grad_u0 + project( dif.gradient( (t/2) * (k1 + k2), x ), grad_psi ), dim=-1, eps=1e-30 )\n",
    "        X_ss = -1 * torch.nn.functional.normalize( proj_grad_u0 + t * project( dif.gradient( F_u0 , x), grad_psi ), dim=-1 , eps=1e-30 )\n",
    "        #X_ss =  torch.nn.functional.normalize( proj_grad_u0 , dim=-1 , eps=1e-30 )\n",
    "        \n",
    "        X[head:min(head + max_batch, amount_samples)] = X_ss.detach().cpu().squeeze(0).numpy()\n",
    "        divX[head:min(head + max_batch, amount_samples)] = dif.divergence( X_ss, x ).detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "        laplacians_psi[head:min(head + max_batch, amount_samples)] = dif.divergence( grad_psi, x ).detach().cpu().squeeze(0).numpy()\n",
    "        grads_psi[head:min(head + max_batch, amount_samples)] = grad_psi.detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "        head += max_batch\n",
    "        i += 1\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return grads_psi, laplacians_psi, uts, X, divX\n",
    "\n",
    "grads_psi, laplacians_psi, ut, X, divX = computeX( model, pc, pc[47,:], u0, grad_u0, max_batch=64**2 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
