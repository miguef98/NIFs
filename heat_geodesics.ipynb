{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat Geodesics en NIFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos la PDE \n",
    "$$\\frac{\\partial u}{\\partial t} = \\nabla \\cdot ( P_{\\nabla \\psi} \\nabla u) = \\Delta u - (\\nabla u \\cdot \\nabla \\psi) \\Delta \\psi$$\n",
    "\n",
    "Definimos $u_0(\\vec{x}) = e^{-\\varepsilon \\left || \\vec{x} - \\vec{p} | \\right |^2}$. Entonces,\n",
    "\n",
    "$$\\nabla u_0(\\vec{x}) = -2 \\varepsilon u_0(\\vec{x}) (\\vec{x} - \\vec{p}) \\quad \\quad \\text{y} \\quad \\quad \\Delta u_0(\\vec{x}) = 2\\varepsilon \\, u_0(\\vec{x}) \\, (2 \\varepsilon \\left || \\vec{x} - \\vec{p} | \\right |^2 - 3) $$\n",
    "\n",
    "### Forward Euler\n",
    "Este esquema es mas sencillo (pero mas inestable), integramos con respecto al tiempo tomando la siguiente aproximacion:\n",
    "$$u_t(\\vec{x}) = u_0(\\vec{x}) + t (\\Delta u_0(\\vec{x}) - (\\nabla u_0 \\cdot \\nabla \\psi) \\Delta \\psi)$$\n",
    "\n",
    "### Backward Euler\n",
    "Integrador implicito:\n",
    "$$u_t = u_0 + t (\\nabla \\cdot \\circ \\, P_{\\nabla \\psi} \\nabla) u_t$$\n",
    "\n",
    "Donde se entiende por $(\\nabla \\cdot \\circ \\, P_{\\nabla \\psi}) : C^k \\rightarrow C^k$ el operador que actua sobre funciones $u$, el cual aplica la proyeccion del gradiente sobre el espacio ortogonal a $\\nabla \\psi$ y luego la divergencia.\n",
    "\n",
    "$$t(id - \\nabla \\cdot \\circ \\, P_{\\nabla \\psi} \\nabla) u_t = u_0$$\n",
    "\n",
    "Entonces, $u_t$ es la funcion que minimiza la energia:\n",
    "\n",
    "$$ \\underset{u: \\Omega \\rightarrow \\mathbb{R}}{\\text{min}} \\quad \\quad \\int_\\Omega \\left | t(id - \\nabla \\cdot \\circ \\, P_{\\nabla \\psi} \\,\\nabla) u_t - u_0 \\right | \\quad d\\vec{x}.$$\n",
    "\n",
    "Tengo muchas formas de realizar esta optimizacion:\n",
    "1. CML dada una familia de funciones\n",
    "2. Discretizacion y FEM ?\n",
    "3. Usando el mismo SIREN\n",
    "\n",
    "Hay que tener fe de que estas minimizaciones ganen ventaja frente a FE... ta dificil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from src.model import SIREN\n",
    "from src.obj import load\n",
    "import trimesh as tm\n",
    "import meshplot as mp\n",
    "import src.diff_operators as dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mesh = tm.load_mesh('results/bunny/experiment/reconstructions/mc_mesh_best.obj')\n",
    "\n",
    "#plot = mp.plot( mesh.vertices, mesh.faces, return_plot=True )\n",
    "#plot.add_points( p, shading={'point_size':0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfainstein/miniconda3/envs/tdf/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SIREN(\n",
       "  (net): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=512, bias=True)\n",
       "      (1): SineLayer(w0=30)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): SineLayer(w0=30)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): SineLayer(w0=30)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): SineLayer(w0=30)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SIREN(\n",
    "        n_in_features= 3,\n",
    "        n_out_features=1,\n",
    "        hidden_layer_config=[512]*4,\n",
    "        w0=30,\n",
    "        ww=None,\n",
    "        activation= 'sine'\n",
    ")\n",
    "model.load_state_dict( torch.load('results/bunny/experiment/models/model_best.pth', weights_only=True))\n",
    "\n",
    "device_torch = torch.device(0)\n",
    "model.to(device_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc( model, samples, max_batch=64**2, device=torch.device(0), iterations = 3 ):\n",
    "    # samples = ( amount_samples, 3 )\n",
    "    amount_samples = samples.shape[0]\n",
    "\n",
    "    head = 0\n",
    "    X = samples.copy()\n",
    "\n",
    "    for i in range( iterations):\n",
    "        print(f'Iteration: {i}')\n",
    "        mean_distance = 0\n",
    "        while head < amount_samples:\n",
    "            \n",
    "            inputs_subset = torch.from_numpy( X[head:min(head + max_batch, amount_samples), :] ).to(device).unsqueeze(0).float()\n",
    "\n",
    "            x, y = model(inputs_subset).values()\n",
    "\n",
    "            mean_distance += torch.sum(y).detach().cpu().numpy()\n",
    "\n",
    "            grad_psi = torch.nn.functional.normalize( dif.gradient(y,x), dim=-1 )\n",
    "\n",
    "            X[head:min(head + max_batch, amount_samples)] -= (y *  grad_psi).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "            head += max_batch\n",
    "\n",
    "        print(f'Mean distance: { mean_distance / amount_samples }')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Mean distance: -9.786325991153717e-05\n"
     ]
    }
   ],
   "source": [
    "EPSILON = 1e-3\n",
    "pc, _ = load( 'data/bunny/bunny_pc.obj' )\n",
    "\n",
    "pc = grad_desc( model, pc, iterations=1 )\n",
    "\n",
    "u0 = lambda x, p:  (torch.exp( -EPSILON * torch.sum( (x - p)**2, dim=-1) ))[...,None]\n",
    "grad_u0 = lambda x, p: -2 * EPSILON * u0(x, p) * (x - p)\n",
    "lap_u0 = lambda x, p: ( 2 * EPSILON * u0(x, p) * ( 2* EPSILON * torch.sum( (x - p)**2, dim=-1) - 3 )[...,None])\n",
    "\n",
    "t = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 1\n",
      "Iteration: 2\n"
     ]
    }
   ],
   "source": [
    "def project( projectees, projectors ):\n",
    "    # asumiendo ||projectors|| = 1\n",
    "    return projectees -  torch.sum(projectors * projectees, dim=-1)[...,None] * projectors\n",
    "\n",
    "def computeX( model, samples, p, u0, grad_u0, max_batch=64**2, device=torch.device(0) ):\n",
    "    # samples = ( amount_samples, 3 )    \n",
    "    head = 0\n",
    "    amount_samples = samples.shape[0]\n",
    "\n",
    "    uts = np.zeros( (amount_samples, 1))\n",
    "    X = np.zeros( (amount_samples, 3))\n",
    "    divX = np.zeros( (amount_samples, 1))\n",
    "\n",
    "    ps = torch.from_numpy( np.tile(p, (max_batch, 1)) ).to(device_torch)\n",
    "    i = 0\n",
    "    while head < amount_samples:\n",
    "        print(f'Iteration: {i}')\n",
    "        \n",
    "        inputs_subset = torch.from_numpy( samples[head:min(head + max_batch, amount_samples), :] ).to(device).unsqueeze(0).float()\n",
    "\n",
    "        x, y =  model(inputs_subset).values()\n",
    "\n",
    "        grad_psi = dif.gradient(y,x)\n",
    "\n",
    "        ps_ss = ps[:inputs_subset.shape[1],:].unsqueeze(0)\n",
    "\n",
    "        u0s = u0( inputs_subset, ps_ss )\n",
    "\n",
    "        # RK4: ( out of memory... lo mata hacer el gradiente de u para calcular X.. es un monton)\n",
    "        #k1 = dif.divergence( project( grad_u0( inputs_subset, ps_ss ), grad_psi ) , x )\n",
    "        #k2 = dif.divergence( project( dif.gradient(u0s + 0.5 * t * k1 , x), grad_psi ), x )\n",
    "        #k3 = dif.divergence( project( dif.gradient(u0s + 0.5 * t * k2 , x), grad_psi ), x )\n",
    "        #k4 = dif.divergence( project( dif.gradient(u0s + t * k3 , x), grad_psi ), x )\n",
    "        #ut = u0( inputs_subset, ps_ss ) + (t/6) * (k1 + 2*k2 + 2*k3 + k4)\n",
    "\n",
    "        # Heuns:\n",
    "        k1 = dif.divergence( project( grad_u0( inputs_subset, ps_ss ), grad_psi ) , x )\n",
    "        k2 = dif.divergence( project( dif.gradient(u0s + t * k1 , x), grad_psi ), x )\n",
    "        ut = u0( inputs_subset, ps_ss ) + (t/2) * (k1 + k2)\n",
    "        \n",
    "        uts[head:min(head + max_batch, amount_samples)] = ut.detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "        #X_ss = -1 * torch.nn.functional.normalize( project( dif.gradient( ut, x ), grad_psi ) , dim=-1 )\n",
    "        X_ss = -1 * torch.nn.functional.normalize(project( grad_u0( inputs_subset, ps_ss ), grad_psi ) , dim=-1 ) #-1 * torch.nn.functional.normalize(  dif.gradient( ut, x ) , dim=-1 )\n",
    "        #X_ss = -1 * project( dif.gradient( ut, x ), grad_psi )\n",
    "\n",
    "        X[head:min(head + max_batch, amount_samples)] = X_ss.detach().cpu().squeeze(0).numpy()\n",
    "        #divX[head:min(head + max_batch, amount_samples)] = dif.divergence( X_ss, x ).detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "        head += max_batch\n",
    "        i += 1\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return uts, X, divX\n",
    "\n",
    "ut, X, divX = computeX( model, pc, pc[10,:], u0, grad_u0, max_batch=64**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9983037662802133, 1.0000000000000029)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(ut), np.max(ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81972cc7c8424d66a8a4e338b4e80914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.1099628…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot = mp.plot( pc, c= ut, shading={'point_size':0.05, 'v_min':0, 'v_max':1}, return_plot=True )\n",
    "plot.add_points( pc[10,:][None,...], shading={'point_size':0.5})\n",
    "plot.add_lines( pc, pc + X * 0.050 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca27206321c48cbb1e776d34642a0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.1099628…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u0_numpy = lambda x, p:  (np.exp( -EPSILON * np.sum( (x - p)**2, axis=-1) ))[...,None]\n",
    "grad_u0_numpy = lambda x, p: -2 * EPSILON * u0_numpy(x, p) * (x - p)\n",
    "\n",
    "u0s = u0_numpy( pc,np.tile(pc[10,:], (len(pc), 1)) )\n",
    "plot = mp.plot( pc, c= u0s, shading={'point_size':0.05, 'v_min':0, 'v_max':1}, return_plot=True )\n",
    "plot.add_points( pc[10,:][None,...], shading={'point_size':0.5})\n",
    "#plot.add_lines( pc, pc + X * 0.05 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5860205329482086e-07"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean( np.abs( u0s - ut ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
