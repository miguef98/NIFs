{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat Geodesics en NIFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos la PDE \n",
    "$$\\frac{\\partial u}{\\partial t} = \\nabla \\cdot ( P_{\\nabla \\psi} \\nabla u) = \\Delta u - (\\nabla u \\cdot \\nabla \\psi) \\Delta \\psi$$\n",
    "\n",
    "Definimos $u_0(\\vec{x}) = e^{-\\varepsilon \\left || \\vec{x} - \\vec{p} | \\right |^2}$. Entonces,\n",
    "\n",
    "$$\\nabla u_0(\\vec{x}) = -2 \\varepsilon u_0(\\vec{x}) (\\vec{x} - \\vec{p}) \\quad \\quad \\text{y} \\quad \\quad \\Delta u_0(\\vec{x}) = 2\\varepsilon \\, u_0(\\vec{x}) \\, (2 \\varepsilon \\left || \\vec{x} - \\vec{p} | \\right |^2 - 3) $$\n",
    "\n",
    "### Forward Euler\n",
    "Este esquema es mas sencillo (pero mas inestable), integramos con respecto al tiempo tomando la siguiente aproximacion:\n",
    "$$u_t(\\vec{x}) = u_0(\\vec{x}) + t (\\Delta u_0(\\vec{x}) - (\\nabla u_0 \\cdot \\nabla \\psi) \\Delta \\psi)$$\n",
    "\n",
    "### Backward Euler\n",
    "Integrador implicito:\n",
    "$$u_t = u_0 + t (\\nabla \\cdot \\circ \\, P_{\\nabla \\psi} \\nabla) u_t$$\n",
    "\n",
    "Donde se entiende por $(\\nabla \\cdot \\circ \\, P_{\\nabla \\psi}) : C^k \\rightarrow C^k$ el operador que actua sobre funciones $u$, el cual aplica la proyeccion del gradiente sobre el espacio ortogonal a $\\nabla \\psi$ y luego la divergencia.\n",
    "\n",
    "$$t(id - \\nabla \\cdot \\circ \\, P_{\\nabla \\psi} \\nabla) u_t = u_0$$\n",
    "\n",
    "Entonces, $u_t$ es la funcion que minimiza la energia:\n",
    "\n",
    "$$ \\underset{u: \\Omega \\rightarrow \\mathbb{R}}{\\text{min}} \\quad \\quad \\int_\\Omega \\left | t(id - \\nabla \\cdot \\circ \\, P_{\\nabla \\psi} \\,\\nabla) u_t - u_0 \\right | \\quad d\\vec{x}.$$\n",
    "\n",
    "Tengo muchas formas de realizar esta optimizacion:\n",
    "1. CML dada una familia de funciones\n",
    "2. Discretizacion y FEM ?\n",
    "3. Usando el mismo SIREN\n",
    "\n",
    "Hay que tener fe de que estas minimizaciones ganen ventaja frente a FE... ta dificil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from src.model import SIREN\n",
    "from src.obj import load\n",
    "import trimesh as tm\n",
    "import meshplot as mp\n",
    "import src.diff_operators as dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array( [[0.5529412, 0.2470588, 0.1302517]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = tm.load_mesh('results/bunny/experiment/reconstructions/mc_mesh_best.obj')\n",
    "\n",
    "plot = mp.plot( mesh.vertices, mesh.faces, return_plot=True )\n",
    "plot.add_points( p, shading={'point_size':0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfainstein/miniconda3/envs/tdf/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SIREN(\n",
       "  (net): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=512, bias=True)\n",
       "      (1): SineLayer(w0=30)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): SineLayer(w0=30)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): SineLayer(w0=30)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): SineLayer(w0=30)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SIREN(\n",
    "        n_in_features= 3,\n",
    "        n_out_features=1,\n",
    "        hidden_layer_config=[512]*4,\n",
    "        w0=30,\n",
    "        ww=None,\n",
    "        activation= 'sine'\n",
    ")\n",
    "model.load_state_dict( torch.load('results/bunny/experiment/models/model_best.pth', weights_only=True))\n",
    "\n",
    "device_torch = torch.device(0)\n",
    "model.to(device_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 0.1\n",
    "pc, _ = load( 'data/bunny/bunny_pc.obj' )\n",
    "\n",
    "u0 = lambda x, p: torch.exp( -EPSILON * torch.sum( (x - p)**2) )\n",
    "grad_u0 = lambda x, p: -2 * EPSILON * u0(x, p) * (x - p)\n",
    "lap_u0 = lambda x, p: 2 * EPSILON * u0(x, p) * ( 2* EPSILON * torch.sum( (x - p)**2) - 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot = mp.plot( mesh.vertices, mesh.faces, return_plot=True )\n",
    "#plot.add_points( pc, shading={'point_size':0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.001\n",
    "pc = torch.from_numpy(pc).to( device_torch ).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (4096) at non-singleton dimension 1.  Target sizes: [4096, 1].  Tensor sizes: [4096, 4096]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m         head \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m max_batch\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uts, X, divX\n\u001b[0;32m---> 35\u001b[0m ut, X, divX \u001b[38;5;241m=\u001b[39m \u001b[43mcomputeX\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_u0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlap_u0\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m, in \u001b[0;36mcomputeX\u001b[0;34m(model, samples, p, u0, grad_u0, lap_u0, max_batch, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m ps_ss \u001b[38;5;241m=\u001b[39m ps[:inputs_subset\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],:]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     23\u001b[0m ut \u001b[38;5;241m=\u001b[39m u0( inputs_subset, ps_ss ) \u001b[38;5;241m+\u001b[39m t \u001b[38;5;241m*\u001b[39m ( lap_u0( inputs_subset, ps_ss ) \u001b[38;5;241m-\u001b[39m ( torch\u001b[38;5;241m.\u001b[39msum( grad_u0( inputs_subset, ps_ss ) \u001b[38;5;241m*\u001b[39m grad_psi, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m ) ) \u001b[38;5;241m*\u001b[39m laplace_psi )\n\u001b[0;32m---> 24\u001b[0m uts[head:\u001b[38;5;28mmin\u001b[39m(head \u001b[38;5;241m+\u001b[39m max_batch, amount_samples)] \u001b[38;5;241m=\u001b[39m ut\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     26\u001b[0m X_ss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize( dif\u001b[38;5;241m.\u001b[39mgradient( ut, x ), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m )\n\u001b[1;32m     28\u001b[0m X[head:\u001b[38;5;28mmin\u001b[39m(head \u001b[38;5;241m+\u001b[39m max_batch, amount_samples)] \u001b[38;5;241m=\u001b[39m X_ss\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (4096) at non-singleton dimension 1.  Target sizes: [4096, 1].  Tensor sizes: [4096, 4096]"
     ]
    }
   ],
   "source": [
    "def computeX( model, samples, p, u0, grad_u0, lap_u0, max_batch=64**2, device=torch.device(0) ):\n",
    "    # samples = ( amount_samples, 3 )    \n",
    "    head = 0\n",
    "    amount_samples = samples.shape[0]\n",
    "\n",
    "    uts = torch.zeros( (amount_samples, 1)).to( device )\n",
    "    X = torch.zeros( (amount_samples, 3)).to( device )\n",
    "    divX = torch.zeros( (amount_samples, 1)).to( device )\n",
    "\n",
    "    ps = torch.from_numpy( np.tile(p, (max_batch, 1)) ).to(device_torch)\n",
    "\n",
    "    while head < amount_samples:\n",
    "        \n",
    "        inputs_subset = samples[head:min(head + max_batch, amount_samples), :].unsqueeze(0)\n",
    "\n",
    "        x, y =  model(inputs_subset).values()\n",
    "\n",
    "        grad_psi = dif.gradient(y,x)\n",
    "        laplace_psi = dif.divergence( grad_psi, x )\n",
    "\n",
    "        ps_ss = ps[:inputs_subset.shape[1],:].unsqueeze(0)\n",
    "\n",
    "        ut = u0( inputs_subset, ps_ss ) + t * ( lap_u0( inputs_subset, ps_ss ) - ( torch.sum( grad_u0( inputs_subset, ps_ss ) * grad_psi, dim=-1 ) ) * laplace_psi )\n",
    "        uts[head:min(head + max_batch, amount_samples)] = ut.squeeze(0)\n",
    "\n",
    "        X_ss = -1 * torch.nn.functional.normalize( dif.gradient( ut, x ), dim=-1 )\n",
    "\n",
    "        X[head:min(head + max_batch, amount_samples)] = X_ss.squeeze(0)\n",
    "        divX[head:min(head + max_batch, amount_samples)] = dif.divergence( X_ss, x ).squeeze(0)\n",
    "\n",
    "        head += max_batch\n",
    "\n",
    "    return uts, X, divX\n",
    "\n",
    "ut, X, divX = computeX( model, pc, p, u0, grad_u0, lap_u0 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
