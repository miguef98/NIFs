{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat Geodesics en NIFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos la PDE \n",
    "$$\\frac{\\partial u}{\\partial t} = \\nabla \\cdot ( P_{\\nabla \\psi} \\nabla u) = \\Delta u - (\\nabla u \\cdot \\nabla \\psi) \\Delta \\psi$$\n",
    "\n",
    "Definimos $u_0(\\vec{x}) = e^{-\\varepsilon \\left || \\vec{x} - \\vec{p} | \\right |^2}$. Entonces,\n",
    "\n",
    "$$\\nabla u_0(\\vec{x}) = -2 \\varepsilon u_0(\\vec{x}) (\\vec{x} - \\vec{p}) \\quad \\quad \\text{y} \\quad \\quad \\Delta u_0(\\vec{x}) = 2\\varepsilon \\, u_0(\\vec{x}) \\, (2 \\varepsilon \\left || \\vec{x} - \\vec{p} | \\right |^2 - 3) $$\n",
    "\n",
    "### Forward Euler\n",
    "Este esquema es mas sencillo (pero mas inestable), integramos con respecto al tiempo tomando la siguiente aproximacion:\n",
    "$$u_t(\\vec{x}) = u_0(\\vec{x}) + t (\\Delta u_0(\\vec{x}) - (\\nabla u_0 \\cdot \\nabla \\psi) \\Delta \\psi)$$\n",
    "\n",
    "De esta forma,\n",
    "$$ \\nabla u_t(\\vec{x}) = \\nabla $$\n",
    "\n",
    "### Backward Euler\n",
    "Integrador implicito:\n",
    "$$u_t = u_0 + t (\\nabla \\cdot \\, P_{\\nabla \\psi} \\nabla) u_t$$\n",
    "\n",
    "Donde se entiende por $(\\nabla \\cdot \\, P_{\\nabla \\psi}) : C^k \\rightarrow C^k$ el operador que actua sobre funciones $u$, el cual aplica la proyeccion del gradiente sobre el espacio ortogonal a $\\nabla \\psi$ y luego la divergencia.\n",
    "\n",
    "$$t(id - \\nabla \\cdot \\, P_{\\nabla \\psi} \\nabla) u_t = u_0$$\n",
    "\n",
    "Entonces, $u_t$ es la funcion que minimiza la energia:\n",
    "\n",
    "$$ \\underset{u: \\Omega \\rightarrow \\mathbb{R}}{\\text{min}} \\quad \\quad \\int_\\Omega \\left | t(id - \\nabla \\cdot \\, P_{\\nabla \\psi} \\,\\nabla) u_t - u_0 \\right | \\quad d\\vec{x}.$$\n",
    "\n",
    "Tengo muchas formas de realizar esta optimizacion:\n",
    "1. CML dada una familia de funciones\n",
    "2. Discretizacion y FEM ?\n",
    "3. Usando el mismo SIREN\n",
    "\n",
    "Hay que tener fe de que estas minimizaciones ganen ventaja frente a FE... ta dificil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from src.model import SIREN\n",
    "from src.obj import load\n",
    "import meshplot as mp\n",
    "import src.diff_operators as dif\n",
    "from torchdiffeq import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfainstein/miniconda3/envs/tdf/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SIREN(\n",
       "  (net): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=512, bias=True)\n",
       "      (1): SineLayer(w0=30)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): SineLayer(w0=30)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): SineLayer(w0=30)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): SineLayer(w0=30)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SIREN(\n",
    "        n_in_features= 3,\n",
    "        n_out_features=1,\n",
    "        hidden_layer_config=[512]*4,\n",
    "        w0=30,\n",
    "        ww=None,\n",
    "        activation= 'sine'\n",
    ")\n",
    "model.load_state_dict( torch.load('results/bunny/experiment/models/model_best.pth', weights_only=True))\n",
    "\n",
    "device_torch = torch.device(0)\n",
    "model.to(device_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Mean distance: -9.786325991153717e-05\n"
     ]
    }
   ],
   "source": [
    "def grad_desc( model, samples, max_batch=64**2, device=torch.device(0), iterations = 3 ):\n",
    "    # samples = ( amount_samples, 3 )\n",
    "    amount_samples = samples.shape[0]\n",
    "\n",
    "    head = 0\n",
    "    X = samples.copy()\n",
    "\n",
    "    for i in range( iterations):\n",
    "        print(f'Iteration: {i}')\n",
    "        mean_distance = 0\n",
    "        while head < amount_samples:\n",
    "            \n",
    "            inputs_subset = torch.from_numpy( X[head:min(head + max_batch, amount_samples), :] ).to(device).unsqueeze(0).float()\n",
    "\n",
    "            x, y = model(inputs_subset).values()\n",
    "\n",
    "            mean_distance += torch.sum(y).detach().cpu().numpy()\n",
    "\n",
    "            grad_psi = torch.nn.functional.normalize( dif.gradient(y,x), dim=-1 )\n",
    "\n",
    "            X[head:min(head + max_batch, amount_samples)] -= (y *  grad_psi).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "            head += max_batch\n",
    "\n",
    "        print(f'Mean distance: { mean_distance / amount_samples }')\n",
    "\n",
    "    return X\n",
    "\n",
    "pc, _ = load( 'data/bunny/bunny_pc.obj' )\n",
    "#pc = pc[np.random.choice( np.arange(10000), 1000),: ]\n",
    "pc = grad_desc( model, pc, iterations=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "EPSILON = 0.001\n",
    "\n",
    "u0 = lambda x, p: torch.exp( - torch.sum( (x - p)**2, dim=-1) / (EPSILON ** 2) )[...,None] / ( EPSILON )\n",
    "grad_u0 = lambda x, p: (-2 / (EPSILON ** 2)) * u0(x, p) * (x - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 1\n",
      "Iteration: 2\n"
     ]
    }
   ],
   "source": [
    "def project( projectees, projectors ):\n",
    "    # asumiendo ||projectors|| = 1\n",
    "    return projectees -  torch.sum(projectors * projectees, dim=-1)[...,None] * projectors\n",
    "\n",
    "def computeX( model, samples, p, u0, grad_u0, max_batch=64**2, device=torch.device(0) ):\n",
    "    # samples = ( amount_samples, 3 )    \n",
    "    head = 0\n",
    "    amount_samples = samples.shape[0]\n",
    "\n",
    "    #uts = np.zeros( (amount_samples, 1))\n",
    "    grads_psi = np.zeros( (amount_samples, 3))\n",
    "    laplacians_psi = np.zeros( (amount_samples, 1))\n",
    "    #X = np.zeros( (amount_samples, 3))\n",
    "    #divX = np.zeros( (amount_samples, 1))\n",
    "\n",
    "    #ps = torch.from_numpy( np.tile(p, (max_batch, 1)) ).to(device_torch)\n",
    "    i = 0\n",
    "    while head < amount_samples:\n",
    "        print(f'Iteration: {i}')\n",
    "        \n",
    "        inputs_subset = torch.from_numpy( samples[head:min(head + max_batch, amount_samples), :] ).to(device).unsqueeze(0).float()\n",
    "\n",
    "        x, y =  model(inputs_subset).values()\n",
    "\n",
    "        grad_psi = dif.gradient(y,x)\n",
    "\n",
    "        #ps_ss = ps[:inputs_subset.shape[1],:].unsqueeze(0)\n",
    "\n",
    "        # Forward Euler\n",
    "        #proj_grad_u0 = project( grad_u0( inputs_subset, ps_ss ), grad_psi )\n",
    "        #F_u0 = dif.divergence( proj_grad_u0 , x )\n",
    "        #u0s = u0( inputs_subset, ps_ss )\n",
    "        #ut = u0s + t * F_u0\n",
    "\n",
    "        # RK4: ( out of memory... lo mata hacer el gradiente de u para calcular X.. es un monton)\n",
    "        #k1 = dif.divergence( project( grad_u0( inputs_subset, ps_ss ), grad_psi ) , x )\n",
    "        #k2 = dif.divergence( project( dif.gradient(u0s + 0.5 * t * k1 , x), grad_psi ), x )\n",
    "        #k3 = dif.divergence( project( dif.gradient(u0s + 0.5 * t * k2 , x), grad_psi ), x )\n",
    "        #k4 = dif.divergence( project( dif.gradient(u0s + t * k3 , x), grad_psi ), x )\n",
    "        #ut = u0s + (t/6) * (k1 + 2*k2 + 2*k3 + k4)\n",
    "\n",
    "        # Heuns:\n",
    "        #k1 = dif.divergence( proj_grad_u0 , x )\n",
    "        #k2 = dif.divergence( project( dif.gradient(u0s + t * k1 , x), grad_psi ), x )\n",
    "        #ut = u0s + (t/2) * (k1 + k2)\n",
    "        \n",
    "        #uts[head:min(head + max_batch, amount_samples)] = ut.detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "        #X_ss = -1 * torch.nn.functional.normalize( proj_grad_u0 + project( dif.gradient( (t/2) * (k1 + k2), x ), grad_psi ), dim=-1, eps=1e-30 )\n",
    "        #X_ss = -1 * torch.nn.functional.normalize( proj_grad_u0 + t * project( dif.gradient( F_u0 , x), grad_psi ), dim=-1 , eps=1e-30 )\n",
    "        #X_ss =  torch.nn.functional.normalize( proj_grad_u0 , dim=-1 , eps=1e-30 )\n",
    "        \n",
    "        #X[head:min(head + max_batch, amount_samples)] = X_ss.detach().cpu().squeeze(0).numpy()\n",
    "        #divX[head:min(head + max_batch, amount_samples)] = dif.divergence( X_ss, x ).detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "        laplacians_psi[head:min(head + max_batch, amount_samples)] = dif.divergence( grad_psi, x ).detach().cpu().squeeze(0).numpy()\n",
    "        grads_psi[head:min(head + max_batch, amount_samples)] = grad_psi.detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "        head += max_batch\n",
    "        i += 1\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return grads_psi, laplacians_psi #, uts, X, divX\n",
    "\n",
    "grads_psi, laplacians_psi = computeX( model, pc, pc[47,:], u0, grad_u0, max_batch=64**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000e+03],\n",
       "          [ 0.0000e+00],\n",
       "          [ 0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00],\n",
       "          [ 0.0000e+00],\n",
       "          [ 0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.9106e+04],\n",
       "          [ 0.0000e+00],\n",
       "          [ 0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00],\n",
       "          [ 0.0000e+00],\n",
       "          [ 0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 2.8476e+07],\n",
       "          [ 0.0000e+00],\n",
       "          [ 0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00],\n",
       "          [ 0.0000e+00],\n",
       "          [ 0.0000e+00]]]], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def F( y, x ):\n",
    "    coords, sdf =  model(x).values()\n",
    "    grad_psi = dif.gradient(sdf, coords)\n",
    "\n",
    "    proj_grad_u0 = project( dif.gradient( y, x ), grad_psi )\n",
    "    return dif.divergence( proj_grad_u0 , x )\n",
    "\n",
    "N = 10000\n",
    "X = torch.from_numpy( pc[:N, :] ).to(device_torch).unsqueeze(0).float()\n",
    "X.requires_grad_()\n",
    "p = torch.from_numpy( np.tile( pc[0,:] , (N, 1)) ).to(device_torch)\n",
    "t = torch.Tensor([0,0.00001, 0.0001]).to(device_torch)\n",
    "t.requires_grad_()\n",
    "\n",
    "u0, u1, u2 = odeint( lambda t, y: F(y, X),u0(X, p) , t, method='euler' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = lambda x : np.clip( x, np.percentile(x, 1), np.percentile(x,99))\n",
    "\n",
    "plot = mp.plot( pc, c= clip(laplacians_psi), shading={'point_size':0.3}, return_plot=True )\n",
    "#plot.add_points( pc[30,:][None,...], shading={'point_size':0.3})\n",
    "plot.add_lines( pc, pc + grads_psi*0.05 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist( clip(laplacians_psi), 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si.... ecuacion de Poisson:\n",
    "\n",
    "Buscamos $\\phi : \\partial \\Omega \\rightarrow \\mathbb{R}$ tal que:\n",
    "\n",
    "$$\\nabla_{\\partial \\Omega} \\phi = X$$\n",
    "\n",
    "Donde $\\nabla_{\\partial \\Omega} = P_{\\nabla \\psi} \\nabla$,   es el gradiente intrinseco de $\\partial \\Omega$. Esto suele ser un problema mal definido (?). Se reemplaza por el clasico problema de Poisson:\n",
    "\n",
    "$$\\Delta_{\\partial \\Omega} \\phi := \\nabla \\cdot \\nabla_{\\partial \\Omega} \\phi = \\nabla \\cdot X $$\n",
    "\n",
    "Entonces la pregunta es como hago para resolver esta ecuacion dado que:\n",
    "1. No tenemos valores en una grilla para hacer algo estilo diferencias finitas.\n",
    "2. No tenemos una malla, por lo tanto no podemos hacer diferencias finitas en malla (como hacen en el paper original) ni elementos finitos.\n",
    "\n",
    "Siento que hay dos opciones principales:\n",
    "1. Olvidarse de la SDF de fondo, y usar los metodos para nubes de puntos que usan en el paper original.\n",
    "2. Pasar a un Octree o algo del estilo y resolver como hacen en PSR.\n",
    "3. *MonteCarlo* ?????\n",
    "4. Usar una red SIREN o RBF.\n",
    "\n",
    "RBF es super facil de implementar definiendo el sistema lineal por cada punto de la nube. El tema es que requiere solucion global (que es ventaja y desventaja)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import RBFInterpolator\n",
    "\n",
    "N = pc.shape[0]\n",
    "G = pc @ pc.T\n",
    "P = -2 * G + np.outer( np.diag(G), np.ones(N)) + np.outer( np.ones(N), np.diag(G))\n",
    "eps = 100\n",
    "\n",
    "phis = np.exp( - eps * P )\n",
    "laplace_Phis = 2 * eps * phis * (2 * eps * P - 3)\n",
    "\n",
    "print(laplace_Phis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = pc @ grads_psi.T\n",
    "\n",
    "D = np.outer( np.diag(W), np.ones(N) ) - W.T\n",
    "\n",
    "A = laplace_Phis #+ ( 2 * eps * phis * D * np.outer( laplacians_psi, np.ones(N)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.linalg.solve( A, divX )#RBFInterpolator(pc, divX,kernel='gaussian', epsilon=eps, smoothing=9)(pc) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.exp( -eps * P ) @ a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min( distances ), np.max(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot( pc, c= distances, shading={'colormap':'Reds' ,'point_size':0.3}, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min( divX) ,np.max(divX), np.mean(divX), np.std(divX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation\n",
    "\n",
    "Vamo de vuelta... tenemos:\n",
    "$$\\frac{\\partial u}{\\partial t} = \\nabla \\cdot ( P_{\\nabla \\psi} \\nabla u) = \\Delta u - (\\nabla u \\cdot \\nabla \\psi) \\Delta \\psi$$\n",
    "\n",
    "... despues lo paso en limpio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = pc.shape[0]\n",
    "G = pc @ pc.T\n",
    "P = -2 * G + np.outer( np.diag(G), np.ones(N)) + np.outer( np.ones(N), np.diag(G))\n",
    "eps = 10\n",
    "\n",
    "M = np.exp( - eps * P )\n",
    "K = 2 * eps * M * (2 * eps * P - 3)\n",
    "\n",
    "GtX = grads_psi @ pc.T\n",
    "T = 2 * eps * M * ( GtX - np.outer( np.diag(GtX), np.ones(N) ) )\n",
    "\n",
    "D_psi = np.diag( laplacians_psi.flatten() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.shape, K.shape, T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, ahora que tengo las matrices tengo que encontrar $\\vec{\\alpha}_0$ que satisfaga la condicion de valor inicial.\n",
    "Creo que en este caso es $\\vec{p} = 1$ y todo el resto en $0$... pero en el paper hacen algo raro. porque en realidad deberia ser la funcion delta? nose... rari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_0,_,_,_ = np.linalg.lstsq( M, np.eye( 1,N, k=47)[0] * 10000)\n",
    "alpha_0[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mp.plot( pc, c= alpha_0, shading={'point_size':0.25}, return_plot=True )\n",
    "p.add_points( pc[47,:][None,...], shading={'point_size':0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.000001\n",
    "A = M - t * (K - D_psi @ T)\n",
    "\n",
    "alpha_t,_,_,_ = np.linalg.lstsq( A, M @ alpha_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot( pc, c= alpha_t, shading={'point_size':0.25} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mmmm no parece andar... no me gusta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Euler con punto fijo\n",
    "\n",
    "$g(u_k) = (h + 1)u_k - h\\nabla \\cdot P_{\\nabla \\psi} \\nabla u_k - u_{k-1}$\n",
    "\n",
    "$u_{k+1} = g(u_k)$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
